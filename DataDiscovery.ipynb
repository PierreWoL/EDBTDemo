{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Discovery and Exploration: State-of-the-art, Challenges and Opportunities\n",
    "## Part 1: Dataset Search\n",
    "### Framework Overview -- D3L\n",
    "#### Input Dataset\n",
    "The input dataset consists of structured data with various attributes. Below is a glimpse of the top 10 rows, showcasing the structure and type of data we are dealing with:\n",
    "\n",
    "| Column1 | Column2 | Column3 |\n",
    "|---------|---------|---------|\n",
    "| Value1  | Value2  | Value3  |\n",
    "| ...     | ...     | ...     |\n",
    "\n",
    "D3L utilizes a comprehensive approach based on:\n",
    "\n",
    "1. **Attribute Header Similarity**\n",
    "2. **Value Similarity**\n",
    "3. **Format Similarity**\n",
    "4. **Value Distribution**\n",
    "#### Output Datasets: Top k searched dataset results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T21:02:20.362416Z",
     "start_time": "2024-03-05T21:02:20.356489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pierre\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generate LSH indexes for all evidence in D3L"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# import and initialize D3L\n",
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "# import pandas as pd\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "threshold = 0.5\n",
    "#  collection of tables\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T21:02:24.743644Z",
     "start_time": "2024-03-05T21:02:23.677758Z"
    }
   },
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading NameIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T21:02:25.819938Z",
     "start_time": "2024-03-05T21:02:25.801503Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name LSH index: LOADED!\n"
     ]
    }
   ],
   "source": [
    "name_lsh = os.path.join(data_path, f'./Name.lsh')\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading FormatIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Format LSH index: LOADED!\n"
     ]
    }
   ],
   "source": [
    "format_lsh = os.path.join(data_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T21:02:27.936169Z",
     "start_time": "2024-03-05T21:02:27.919724Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading ValueIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value LSH index: LOADED!\n"
     ]
    }
   ],
   "source": [
    "value_lsh = os.path.join(data_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T21:02:28.967770Z",
     "start_time": "2024-03-05T21:02:28.964087Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading ValueIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution LSH index: LOADED!\n"
     ]
    }
   ],
   "source": [
    "   # DistributionIndex\n",
    "distribution_lsh = os.path.join(data_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T21:02:30.003463Z",
     "start_time": "2024-03-05T21:02:29.990169Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading EmbeddingIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists. Use --overwrite to download anyway.\n",
      "Loading embeddings. This may take a few minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 227.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding LSH index: SAVED!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embed_name = './embedding_.lsh'\n",
    "embedding_lsh = os.path.join(data_path, embed_name)\n",
    "\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T21:04:31.947671Z",
     "start_time": "2024-03-05T21:02:31.271806Z"
    }
   },
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### show the input table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T21:07:46.834816Z",
     "start_time": "2024-03-05T21:07:46.810603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Rank                                 Title      Category Publisher\n",
      "0     1               Super Smash Bros. Melee      Fighting  Nintendo\n",
      "1     2                              Pikmin 2  Strategy/Sim  Nintendo\n",
      "2     3  Legend of Zelda: Collector's Edition           RPG  Nintendo\n"
     ]
    }
   ],
   "source": [
    "searched_table = 'T2DV2_122'\n",
    "table_df = dataloader.read_table(searched_table)\n",
    "print(table_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('T2DV2_122', [1.0, 0.75, 0.0, 0.0, 0.25]),\n ('T2DV2_145', [1.0, 0.3063399041022909, 0.0, 0.0, 0.25]),\n ('T2DV2_55', [1.0, 0.0, 0.0, 0.0, 0.759765625]),\n ('T2DV2_214', [1.0, 0.0, 0.0, 0.0, 0.609375]),\n ('T2DV2_221', [1.0, 0.0, 0.0, 0.0, 0.6064453125]),\n ('T2DV2_205', [1.0, 0.0, 0.0, 0.0, 0.6064453125]),\n ('T2DV2_81', [1.0, 0.0, 0.0, 0.0, 0.5908203125]),\n ('T2DV2_77', [1.0, 0.0, 0.0, 0.0, 0.546875]),\n ('T2DV2_22', [1.0, 0.0, 0.0, 0.0, 0.546875]),\n ('T2DV2_146', [1.0, 0.0, 0.0, 0.0, 0.537109375])]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Searched results, K =10\n",
    "qe = QueryEngine(name_index, format_index, value_index, embedding_index, distribution_index)\n",
    "results, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table),\n",
    "                                           aggregator=None, k=10, verbose=True)\n",
    "results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T21:07:51.348784Z",
     "start_time": "2024-03-05T21:07:51.194772Z"
    }
   },
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-05T21:07:54.439205Z",
     "start_time": "2024-03-05T21:07:54.105564Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Individual search results\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;66;03m# Name index query\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m name_results \u001B[38;5;241m=\u001B[39m \u001B[43mname_index\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<string>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m<integer>\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# The query arg should be a column name. Tokenization will be performed automatically.\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m# Format index query\u001B[39;00m\n\u001B[0;32m      6\u001B[0m format_results \u001B[38;5;241m=\u001B[39m format_index\u001B[38;5;241m.\u001B[39mquery(query\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<list/set>\u001B[39m\u001B[38;5;124m\"\u001B[39m, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<integer>\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;66;03m# The query arg should be a collection of string values. The corresponding format descriptors will be extracted automatically.\u001B[39;00m\n",
      "File \u001B[1;32mE:\\Project\\EDBTDemo-main\\d3l\\indexing\\similarity_indexes\\__init__.py:183\u001B[0m, in \u001B[0;36mNameIndex.query\u001B[1;34m(self, query, k)\u001B[0m\n\u001B[0;32m    181\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(query_signature) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m []\n\u001B[1;32m--> 183\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlsh_index\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mquery\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquery_signature\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwith_scores\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\n\u001B[0;32m    185\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mE:\\Project\\EDBTDemo-main\\d3l\\indexing\\lsh\\lsh_index.py:425\u001B[0m, in \u001B[0;36mLSHIndex.query\u001B[1;34m(self, query_id, query, k, with_scores)\u001B[0m\n\u001B[0;32m    418\u001B[0m neighbours \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    419\u001B[0m     n\n\u001B[0;32m    420\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m hash_entry, hash_table \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(hash_chunks, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_hashtables)\n\u001B[0;32m    421\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m hash_table\u001B[38;5;241m.\u001B[39mget(hash_entry, [])\n\u001B[0;32m    422\u001B[0m ]\n\u001B[0;32m    424\u001B[0m neighbour_counter \u001B[38;5;241m=\u001B[39m Counter(neighbours)\n\u001B[1;32m--> 425\u001B[0m neighbours \u001B[38;5;241m=\u001B[39m [w \u001B[38;5;28;01mfor\u001B[39;00m w, _ \u001B[38;5;129;01min\u001B[39;00m \u001B[43mneighbour_counter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmost_common\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mif\u001B[39;00m w \u001B[38;5;241m!=\u001B[39m query_id]\n\u001B[0;32m    426\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_scores:\n\u001B[0;32m    427\u001B[0m     similarity_scores \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m    428\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_similarity_score(query_hash, n)\n\u001B[0;32m    429\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m query_id \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    430\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_similarity_score(query_id, n)\n\u001B[0;32m    431\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m n \u001B[38;5;129;01min\u001B[39;00m neighbours\n\u001B[0;32m    432\u001B[0m     ]\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\collections\\__init__.py:622\u001B[0m, in \u001B[0;36mCounter.most_common\u001B[1;34m(self, n)\u001B[0m\n\u001B[0;32m    620\u001B[0m \u001B[38;5;66;03m# Lazy import to speedup Python startup time\u001B[39;00m\n\u001B[0;32m    621\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mheapq\u001B[39;00m\n\u001B[1;32m--> 622\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mheapq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnlargest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_itemgetter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\heapq.py:542\u001B[0m, in \u001B[0;36mnlargest\u001B[1;34m(n, iterable, key)\u001B[0m\n\u001B[0;32m    540\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    541\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 542\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m:\n\u001B[0;32m    543\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(iterable, key\u001B[38;5;241m=\u001B[39mkey, reverse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)[:n]\n\u001B[0;32m    545\u001B[0m \u001B[38;5;66;03m# When key is none, use simpler decoration\u001B[39;00m\n",
      "\u001B[1;31mTypeError\u001B[0m: '>=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "#Individual search results\n",
    "# Name index query\n",
    "name_results = name_index.query(query=\"<string>\", k=\"<integer>\") # The query arg should be a column name. Tokenization will be performed automatically.\n",
    "\n",
    "# Format index query\n",
    "format_results = format_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of string values. The corresponding format descriptors will be extracted automatically.\n",
    "\n",
    "# Value index query\n",
    "value_results = value_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of string values. Value pre-processing will be performed automatically.\n",
    "\n",
    "# Embeddings index query\n",
    "embedding_results = embedding_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of string values. The corresponding embeddings will be extracted automatically.\n",
    "\n",
    "# Distribution index query\n",
    "distribution_results = distribution_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of numerical values. The corresponding distribution will be extracted automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Dataset Navigation\n",
    "### Framework Overview -- Aurum\n",
    "TBC Still under work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Dataset Annotation\n",
    "### Framework Overview -- TableMiner+\n",
    "#### Input dataset: 26 tables from 13 domain, while each domain has 2 tables (TBC)\n",
    "The 13 domains include:\n",
    "1. **Airport**\n",
    "2. **City**\n",
    "3. **CollegeOrUniversity**\n",
    "4. **Company**\n",
    "5. **Continent**\n",
    "6. **Country**\n",
    "7. **Hospital**\n",
    "8. **LandmarksOrHistoricalBuildings**\n",
    "9. **Monarch**\n",
    "10. **Movie**\n",
    "11. **Museum**\n",
    "12. **Scientist**\n",
    "13. **VideoGame**\n",
    "\n",
    "TableMiner+ has 4 steps:\n",
    "1. **Subject Column Detection: Including column (data) type detection** \n",
    "2. **NE-Column interpretation - the LEARNING phase:**\n",
    "***2.1 preliminary cell annotation***\n",
    "***2.2 column semantic type annotation***\n",
    "***2.3 property annotation***\n",
    "3. **NE-Column interpretation - the UPDATE phase: revise annotation until all annotation is stabilized**\n",
    "4. **Relation enumeration and annotating literal-columns(not included yet)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### show the example annotation table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase\n",
    "Table = pd.read_csv(\"E:\\Project\\EDBTDemo\\Datasets\\T2DV2_122.csv\") #125\n",
    "print(Table, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Perform NE-Column interpretation (Table Learning includes the process of subject column detection of a table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tableLearning = TableLearning(Table)\n",
    "tableLearning.table_learning()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Perform NE-Column interpretation - the UPDATE phase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "updatePhase(tableLearning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### check the annotation of column (this needs re-factor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "annotation_class = tableLearning.get_annotation_class()\n",
    "for column_index, learning_class in annotation_class.items():\n",
    "    column = learning_class.get_column()\n",
    "    print(f\"column is {column}\")\n",
    "    cellAnnotation  = learning_class.get_cell_annotation()\n",
    "    ColumnSemantics = learning_class.get_winning_concepts()\n",
    "    print(f\"Cell annotation of the column: {cellAnnotation}\")\n",
    "    print(f\"Column semantic type of the column: {ColumnSemantics}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 4: Schema Inference\n",
    "### Framework Overview -- Starmie\n",
    "#### Input dataset: all 200 tables covering 13 specific domains\n",
    "Starmie perform column clustering on the embedding of each column.\n",
    "Embedding generation: Description ...  "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
