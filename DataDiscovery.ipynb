{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Dataset Discovery and Exploration: State-of-the-art, Challenges and Opportunities\n",
    "## Part 1: Dataset Search\n",
    "### Framework Overview -- D3L\n",
    "#### Input Dataset\n",
    "The input dataset consists of structured data with various attributes. Below is a glimpse of the top 10 rows, showcasing the structure and type of data we are dealing with:\n",
    "\n",
    "| Column1 | Column2 | Column3 |\n",
    "|---------|---------|---------|\n",
    "| Value1  | Value2  | Value3  |\n",
    "| ...     | ...     | ...     |\n",
    "\n",
    "D3L utilizes a comprehensive approach based on:\n",
    "\n",
    "1. **Attribute Header Similarity**\n",
    "2. **Value Similarity**\n",
    "3. **Format Similarity**\n",
    "4. **Value Distribution**\n",
    "5. **Attribute value embeddings**\n",
    "#### Output Datasets: Top k searched dataset results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generate LSH indexes for all evidence in D3L"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# import and initialize D3L\n",
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "#\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result\"\n",
    "threshold = 0.5\n",
    "#  collection of tables\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading NameIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result\\Name.lsh\n",
      "Name LSH index: LOADED!\n"
     ]
    }
   ],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading FormatIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading ValueIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value LSH index: LOADED!\n"
     ]
    }
   ],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading DistributionIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "   # DistributionIndex\n",
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Generating/loading EmbeddingIndex of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "embed_name = './embedding_.lsh'\n",
    "embedding_lsh = os.path.join(result_path, embed_name)\n",
    "\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### show the input table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searched_table = 'T2DV2_122'\n",
    "table_df = dataloader.read_table(searched_table)\n",
    "print(table_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Query table in the framework using all the above indexes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Searched results, K =10\n",
    "qe = QueryEngine(name_index, format_index, value_index, embedding_index, distribution_index)\n",
    "results, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table),\n",
    "                                           aggregator=None, k=10, verbose=True)\n",
    "print(results)\n",
    "table_df = dataloader.read_table(results[1][0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(table_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Individual search using different methods"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual search results\n",
    "# Name index query\n",
    "name_results = name_index.query(query=\"<string>\", k=\"<integer>\") # The query arg should be a column name. Tokenization will be performed automatically.\n",
    "\n",
    "# Format index query\n",
    "format_results = format_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of string values. The corresponding format descriptors will be extracted automatically.\n",
    "\n",
    "# Value index query\n",
    "value_results = value_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of string values. Value pre-processing will be performed automatically.\n",
    "\n",
    "# Embeddings index query\n",
    "embedding_results = embedding_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of string values. The corresponding embeddings will be extracted automatically.\n",
    "\n",
    "# Distribution index query\n",
    "distribution_results = distribution_index.query(query=\"<list/set>\", k=\"<integer>\") # The query arg should be a collection of numerical values. The corresponding distribution will be extracted automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 2: Dataset Navigation\n",
    "### Framework Overview -- Aurum"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Prerequisites: detect subject columns and type of columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "import pandas as pd\n",
    "import os\n",
    "table_names = os.listdir(\"Datasets\")\n",
    "table_dict = {}\n",
    "for table_name in table_names:\n",
    "    table_dict[table_name] = []\n",
    "    table = pd.read_csv(f\"Datasets/{table_name}\")\n",
    "    try:\n",
    "        annotation = TA(table)\n",
    "        annotation.subcol_Tjs()\n",
    "        table_dict[table_name].append(annotation.annotation)\n",
    "        table_dict[table_name].append(annotation.column_score)\n",
    "    except:\n",
    "        print(f\"table {table_name} fails\")\n",
    "with open(\"Result/dict.pkl\", \"wb\") as save_file:\n",
    "    pickle.dump(table_dict, save_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"T2DV2_1.csv.Fans' Rank\", 0     442\n",
      "1     267\n",
      "2     505\n",
      "3     175\n",
      "4     486\n",
      "     ... \n",
      "95     44\n",
      "96     78\n",
      "97     74\n",
      "98    495\n",
      "99    281\n",
      "Name: Fans' Rank, Length: 100, dtype: int64)\n",
      "T2DV2_162.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_137.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_100.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_75.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_235.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_133.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_1.Fans' Rank [1.0, 0.0]\n",
      "T2DV2_77.Rank [0.524, 0.0]\n",
      "T2DV2_187.Rank [0.524, 0.0]\n",
      "T2DV2_55.Rank [0.524, 0.0]\n",
      "T2DV2_205.Rank [0.524, 0.0]\n",
      "T2DV2_81.Rank [0.524, 0.0]\n",
      "T2DV2_19.Rank [0.524, 0.0]\n",
      "T2DV2_22.Rank [0.524, 0.0]\n",
      "T2DV2_221.Rank [0.524, 0.0]\n",
      "T2DV2_146.Rank [0.524, 0.0]\n",
      "T2DV2_203.Rank [0.524, 0.0]\n",
      "T2DV2_214.Rank [0.524, 0.0]\n",
      "T2DV2_193.Rank [0.524, 0.0]\n",
      "T2DV2_122.Rank [0.524, 0.0]\n",
      "T2DV2_164.Rank [0.524, 0.0]\n",
      "T2DV2_145.Rank [0.524, 0.0]\n"
     ]
    }
   ],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 3: Dataset Annotation\n",
    "### Framework Overview -- TableMiner+\n",
    "#### Input dataset: 26 tables from 13 domain, while each domain has 2 tables (TBC)\n",
    "The 13 domains include:\n",
    "1. **Airport**\n",
    "2. **City**\n",
    "3. **CollegeOrUniversity**\n",
    "4. **Company**\n",
    "5. **Continent**\n",
    "6. **Country**\n",
    "7. **Hospital**\n",
    "8. **LandmarksOrHistoricalBuildings**\n",
    "9. **Monarch**\n",
    "10. **Movie**\n",
    "11. **Museum**\n",
    "12. **Scientist**\n",
    "13. **VideoGame**\n",
    "\n",
    "TableMiner+ has 4 steps:\n",
    "1. **Subject Column Detection: Including column (data) type detection** \n",
    "2. **NE-Column interpretation - the LEARNING phase:**\n",
    "***2.1 preliminary cell annotation***\n",
    "***2.2 column semantic type annotation***\n",
    "***2.3 property annotation***\n",
    "3. **NE-Column interpretation - the UPDATE phase: revise annotation until all annotation is stabilized**\n",
    "4. **Relation enumeration and annotating literal-columns(not included yet)**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### show the example annotation table"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase\n",
    "Table = pd.read_csv(\"E:\\Project\\EDBTDemo\\Datasets\\T2DV2_122.csv\") #125\n",
    "print(Table, \"\\n\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Perform NE-Column interpretation (Table Learning includes the process of subject column detection of a table)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "tableLearning = TableLearning(Table)\n",
    "tableLearning.table_learning()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Perform NE-Column interpretation - the UPDATE phase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "updatePhase(tableLearning)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### check the annotation of column (this needs re-factor)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "annotation_class = tableLearning.get_annotation_class()\n",
    "for column_index, learning_class in annotation_class.items():\n",
    "    column = learning_class.get_column()\n",
    "    print(f\"column is {column}\")\n",
    "    cellAnnotation  = learning_class.get_cell_annotation()\n",
    "    ColumnSemantics = learning_class.get_winning_concepts()\n",
    "    print(f\"Cell annotation of the column: {cellAnnotation}\")\n",
    "    print(f\"Column semantic type of the column: {ColumnSemantics}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Part 4: Schema Inference\n",
    "### Framework Overview -- Starmie\n",
    "#### Input dataset: all 200 tables covering 13 specific domains\n",
    "Starmie perform column clustering on the embedding of each column.\n",
    "Embedding generation: Description ...  "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Use clustering on table's embeddings to detect the types/domains' of tables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from clustering import typeInference,result_precision,conceptualAttri,current_dir_path\n",
    "clustering_result = typeInference(\"Result\\\\tableEmbeddings.pkl\", \"Agglomerative\", numEstimate=13)\n",
    "print(clustering_result)\n",
    "result_precision = result_precision(clustering_result)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Use clustering on column embeddings to detect column's type "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "column_clustering = conceptualAttri(os.path.join(current_dir_path, \"Datasets\"),\n",
    "                os.path.join(current_dir_path, \"Result/tableEmbeddings.pkl\"),\n",
    "                clustering_method=\"Agglomerative\",\n",
    "                numEstimate=12)\n",
    "for column_index, column_clusters in column_clustering.items():\n",
    "    print(column_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "project_specific_kernel",
   "language": "python",
   "display_name": "Project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
