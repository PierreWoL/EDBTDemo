{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Dataset Discovery and Exploration: State-of-the-art, Challenges and Opportunities\n",
    "## Part 1: Dataset Search\n",
    "### Framework Overview -- D3L\n",
    "\n",
    "\n",
    "This project utilizes structured data derived from the Web Data Commons project, focusing on:\n",
    "- **T2Dv2 Gold Standard for Matching Web Tables to DBpedia**: 108 tables from 9 entity classes. [Access here](https://webdatacommons.org/webtables/goldstandardV2.html).\n",
    "- **Schema.org Table Corpus 2023**: 92 tables from 8 entity classes. [Access here](https://webdatacommons.org/structureddata/schemaorgtables/2023/index.html#toc3).\n",
    "#### Input Dataset\n",
    "The input dataset consists of structured data with various attributes. Below is a glimpse of the top 5 rows, showcasing the structure and type of data we are dealing with:\n",
    "\n",
    "| Rank | Title                                | Category         | Publisher |\n",
    "|------|--------------------------------------|------------------|-----------|\n",
    "| 1    | Super Smash Bros. Melee              | Fighting         | Nintendo  |\n",
    "| 2    | Pikmin 2                             | Strategy/Sim     | Nintendo  |\n",
    "| 3    | Legend of Zelda: Collector's Edition | RPG              | Nintendo  |\n",
    "| 4    | Legend of Zelda: The Wind Waker      | Action Adventure | Nintendo  |\n",
    "| 5    | Metal Gear Solid: Twin Snakes        | Action Adventure | Konami    |\n",
    "\n",
    "\n",
    "\n",
    "D3L utilizes a comprehensive approach based on:\n",
    "\n",
    "1. **Attribute Header Similarity**\n",
    "2. **Value Similarity**\n",
    "3. **Format Similarity**\n",
    "4. **Value Distribution**\n",
    "5. **Attribute value embeddings**\n",
    "#### Output Datasets: Top k searched dataset results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Download required words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generate LSH indexes for all evidence in D3L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Utils import mkdir\n",
    "# import and initialize D3L\n",
    "from d3l.indexing.similarity_indexes import NameIndex, FormatIndex, ValueIndex, EmbeddingIndex, DistributionIndex\n",
    "from d3l.input_output.dataloaders import CSVDataLoader\n",
    "from d3l.querying.query_engine import QueryEngine\n",
    "from d3l.utils.functions import pickle_python_object, unpickle_python_object\n",
    "import os\n",
    "#\n",
    "\n",
    "data_path = \"Datasets\"\n",
    "result_path = \"Result/\"\n",
    "threshold = 0.5\n",
    "mkdir(result_path)\n",
    "#  collection of tables\n",
    "\n",
    "dataloader = CSVDataLoader(\n",
    "        root_path=data_path,\n",
    "        encoding='utf-8'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading NameIndex of tables\n",
    "Name index: Use q-gram analysis of attribute names to calculate the Jaccard distance between their qsets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_lsh = os.path.join(result_path, 'Name.lsh')\n",
    "print(name_lsh)\n",
    "if os.path.isfile(name_lsh):\n",
    "    name_index = unpickle_python_object(name_lsh)\n",
    "    print(\"Name LSH index: LOADED!\")\n",
    "else:\n",
    "    name_index = NameIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(name_index, name_lsh)\n",
    "    print(\"Name LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading FormatIndex of tables\n",
    " Format Index: Identifies data formats through regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "format_lsh = os.path.join(result_path, './format.lsh')\n",
    "if os.path.isfile(format_lsh):\n",
    "    format_index = unpickle_python_object(format_lsh)\n",
    "    print(\"Format LSH index: LOADED!\")\n",
    "else:\n",
    "    format_index = FormatIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(format_index, format_lsh)\n",
    "    print(\"Format LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading ValueIndex of tables\n",
    "Value Index: Employs TFIDF tokens to represent values, with Jaccard distance between their t-sets assessing similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "value_lsh = os.path.join(result_path, './value.lsh')\n",
    "if os.path.isfile(value_lsh):\n",
    "    value_index = unpickle_python_object(value_lsh)\n",
    "    print(\"Value LSH index: LOADED!\")\n",
    "else:\n",
    "    value_index = ValueIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(value_index, value_lsh)\n",
    "    print(\"Value LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading DistributionIndex of tables\n",
    "Distribution Index: Assesses numeric attribute value relatedness via the Kolmogorov-Smirnov statistic, offering insights into domain-originating samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "   # DistributionIndex\n",
    "distribution_lsh = os.path.join(result_path, './distribution.lsh')\n",
    "if os.path.isfile(distribution_lsh):\n",
    "    distribution_index = unpickle_python_object(distribution_lsh)\n",
    "    print(\"Distribution LSH index: LOADED!\")\n",
    "else:\n",
    "    distribution_index = DistributionIndex(dataloader=dataloader, index_similarity_threshold=threshold)\n",
    "    pickle_python_object(distribution_index, distribution_lsh)\n",
    "    print(\"Distribution LSH index: SAVED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Generating/loading EmbeddingIndex of tables\n",
    "Embedding index: Determines textual content relatedness through cosine distance of their vector representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "embed_name = './embedding.lsh'\n",
    "embedding_lsh = os.path.join(result_path, embed_name)\n",
    "if os.path.isfile(embedding_lsh):\n",
    "    embedding_index = unpickle_python_object(embedding_lsh)\n",
    "    print(\"Embedding LSH index: LOADED!\")\n",
    "else:\n",
    "    embedding_index = EmbeddingIndex(dataloader=dataloader,\n",
    "                                     index_similarity_threshold=threshold)\n",
    "    pickle_python_object(embedding_index, embedding_lsh)\n",
    "    print(\"Embedding LSH index: SAVED!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### show the input table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "searched_table = 'T2DV2_122'\n",
    "table_df = dataloader.read_table(searched_table)\n",
    "print(table_df.head(5))\n",
    "df = pd.read_csv(\"groundTruth.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Query table in the framework using all the above indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Searched results, K =10\n",
    "qe = QueryEngine(name_index, value_index, embedding_index, format_index,  distribution_index)\n",
    "results, extended_results = qe.table_query(table=dataloader.read_table(table_name=searched_table),\n",
    "                                           aggregator=None, k=10, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Output the results and check if the output tables have the same type as the input query table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Summarize searched results in a table\n",
    "class_input_table = df[df['fileName'] == searched_table+\".csv\"]['class'].tolist()[0]\n",
    "\n",
    "data = []\n",
    "exceptions = []\n",
    "average = []\n",
    "for table, score in results:\n",
    "        class_table = df[df['fileName'] == table+\".csv\"]['class'].tolist()[0]\n",
    "        data.append((table, score,class_table))\n",
    "        average.append(sum(score)/len(score))\n",
    "        if class_table!=class_input_table:\n",
    "            exceptions.append(table)\n",
    "# Creating the DataFrame\n",
    "result_summarization = pd.DataFrame(data, columns=[\"Table Name\", \"Scores\", \"Ground Truth Class\"])\n",
    "result_summarization = pd.concat([result_summarization.drop([\"Scores\"], axis=1), result_summarization[\"Scores\"].apply(pd.Series)], axis=1).round(3)\n",
    "result_summarization.columns = [\"Table Name\", \"Class\", \"Header Score\", \"Value Score\", \"Embedding Score\",\"Format Score\",  \"Distribution Score\"]\n",
    "result_summarization[\"average score\"] = average\n",
    "print(result_summarization)\n",
    "print(result_summarization[\"Table Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### For tables that does not belong to the same class of input table, show the specific table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for table_name in exceptions:\n",
    "    table_except = dataloader.read_table(table_name)\n",
    "    table_except_part = table_except.head(5)\n",
    "    print(table_except_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Individual search using different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Individual search results\n",
    "# Name index query\n",
    "topk = 10\n",
    "def remove_search_col(listA, check_col):\n",
    "    return [i for i, score in listA if i!=check_col]\n",
    "        \n",
    "def check_column(Dataloader:CSVDataLoader, combined_column_name):\n",
    "    table_name, column_name = combined_column_name.split(\".\")\n",
    "    table = Dataloader.read_table(table_name)\n",
    "    return table[column_name]\n",
    "\n",
    "def table_results(list_result):\n",
    "    return pd.DataFrame(list_result, columns=[\"Column Name\", \"Scores\"])\n",
    "\n",
    "name_results = name_index.query(query=\"Title\", k=topk)\n",
    "print(f\"Name results are \\n {table_results(name_results)} \\n\")\n",
    "\n",
    "# Value index query\n",
    "value_results = value_index.query(query=table_df[\"Title\"], k=topk)\n",
    "columns = [check_column(dataloader, column) for column,score in value_results if column !=\"T2DV2_122.Title\" ]\n",
    "print(f\"Value indexes results are \\n {table_results(value_results)}\\n\")\n",
    "print(f\"example results searching Attribute value indexes:\\n {columns[0]} \\n\")\n",
    "\n",
    "# Embeddings index query\n",
    "embedding_results = embedding_index.query(query=table_df[\"Title\"], k=topk)\n",
    "print(f\"Embedding indexes results are \\n{table_results(embedding_results)} \\n\")\n",
    "embedding_column  = [check_column(dataloader, column) for column,score in embedding_results if column !=\"T2DV2_122.Title\" ]\n",
    "print(f\"example results searching embedding value indexes:\\n {embedding_column[0]} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 2: Dataset Navigation\n",
    "### Framework Overview -- Aurum\n",
    "This is a simplified version of Aurum. It includes two phases: signature building stage and relationship building stage.\n",
    "Signatures: LSH indexes from D3L: name index and value index\n",
    "Relationship Building Stage: Search similar columns based on similarity in name and value LSH indexes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Prerequisites: detect subject columns and type of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from TableMiner.SCDection.TableAnnotation import TableColumnAnnotation as TA\n",
    "\"\"\"\n",
    "Find the column type and Named entity scores in each table,\n",
    " store the table and related column type/NE-scores info as dict in pickle file\n",
    "\"\"\"\n",
    "def subjectColDetection(DATA_PATH,RESULT_PATH):\n",
    "    table_dict = {}\n",
    "    if \"dict.pkl\" in os.listdir(RESULT_PATH):\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"rb\") as f:\n",
    "            table_dict = pickle.load(f)\n",
    "    else:\n",
    "        table_names = os.listdir(DATA_PATH)\n",
    "        for tableName in table_names:\n",
    "            table_dict[tableName] = []\n",
    "            table_ori = pd.read_csv(f\"Datasets/{tableName}\")\n",
    "            try:\n",
    "                annotation_table = TA(table_ori, SearchingWeb = False)\n",
    "                annotation_table.subcol_Tjs()\n",
    "                table_dict[tableName].append(annotation_table.annotation)\n",
    "                table_dict[tableName].append(annotation_table.column_score)\n",
    "            except:\n",
    "                continue\n",
    "        with open(os.path.join(RESULT_PATH,\"dict.pkl\"), \"wb\") as save_file:\n",
    "            pickle.dump(table_dict, save_file)\n",
    "    return table_dict\n",
    "SubjectCol_dict = subjectColDetection(data_path,\"Result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Find the subject columns of result tables from Part I Dataset search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_tables = df[df['class'] == class_input_table]['fileName'].tolist()\n",
    "subject_columns=[]\n",
    "all_columns = []\n",
    "\"\"\"\n",
    "Use iteration and the above column info dict to find the subject columns (and all columns)\n",
    " in each table.\n",
    "\"\"\"\n",
    "for table in result_tables:\n",
    "    df_table = dataloader.read_table(table[:-4])\n",
    "    annotation, NE_column_score = SubjectCol_dict[table]\n",
    "    max_score = max(NE_column_score.values())\n",
    "    all_columns.extend([f\"{table[:-4]}.{df_table.columns[i]}\" for i in NE_column_score.keys()])\n",
    "    subcol_index = [key for key, value in NE_column_score.items() if value == max_score]\n",
    "    for index in subcol_index:\n",
    "        subject_columns.append(f\"{table[:-4]}.{df_table.columns[index]}\")\n",
    "print(subject_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from Aurum.graph import buildGraph,draw_interactive_network\n",
    "# Use Aurum to build the graph\n",
    "G = buildGraph(dataloader, data_path, [name_index, value_index],target_path=\"Result\", table_dict=SubjectCol_dict)\n",
    "import networkx as nx\n",
    "\"\"\"\n",
    "Find the subgraph in the Aurum that contains the provided nodes and all the nodes that\n",
    "have routine to these nodes\n",
    "\"\"\"\n",
    "def subgraph(given_nodes,graph:nx.Graph()):\n",
    "    # Find the connected components containing the given node\n",
    "    subgraphs = list(nx.connected_components(graph))\n",
    "    relevant_nodes = set()\n",
    "    for node in given_nodes:\n",
    "        for sg in subgraphs:\n",
    "            if node in sg:\n",
    "                relevant_nodes.update(sg)\n",
    "    new_graph = G.subgraph(relevant_nodes).copy()\n",
    "    return new_graph\n",
    "subject_columns_graph = subgraph(subject_columns, G)\n",
    "result_SC_graph = subgraph(subject_columns, G)\n",
    "draw_interactive_network(result_SC_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# See all columns in the graph\n",
    "result_graph = subgraph(all_columns, G)\n",
    "draw_interactive_network(result_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 3: Dataset Annotation\n",
    "### Framework Overview -- TableMiner+\n",
    "#### Input dataset: 13 tables from 13 domain, while each domain has 1 table\n",
    "The 13 domains include:\n",
    "1. **Airport**\n",
    "2. **City**\n",
    "3. **CollegeOrUniversity**\n",
    "4. **Company**\n",
    "5. **Continent**\n",
    "6. **Country**\n",
    "7. **Hospital**\n",
    "8. **LandmarksOrHistoricalBuildings**\n",
    "9. **Monarch**\n",
    "10. **Movie**\n",
    "11. **Museum**\n",
    "12. **Scientist**\n",
    "13. **VideoGame**\n",
    "\n",
    "TableMiner+ has 4 steps:\n",
    "1. **Subject Column Detection: Including column (data) type detection** \n",
    "2. **NE-Column interpretation - the LEARNING phase:**\n",
    "***2.1 preliminary cell annotation***\n",
    "***2.2 column semantic type annotation***\n",
    "***2.3 property annotation***\n",
    "3. **NE-Column interpretation - the UPDATE phase: revise annotation until all annotation is stabilized**\n",
    "4. **Relation enumeration and annotating literal-columns(not included yet)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### show the example annotation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from TableMiner.LearningPhase.Update import TableLearning,  updatePhase\n",
    "### The 13 tables from 13 different domains in the dataset\n",
    "table_domains = [\"Test_corpus_105\", \"T2DV2_168\",\n",
    "                 \"Test_corpus_72\",\"T2DV2_22\",\n",
    "                 \"Test_corpus_90\",\"T2DV2_155\",\n",
    "                 \"T2DV2_191\",\"Test_corpus_135\",\n",
    "                 \"T2DV2_30\",\"T2DV2_162\",\n",
    "                 \"T2DV2_109\",\"T2DV2_121\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Perform NE-Column interpretation (Table Learning includes the process of subject column detection of a table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def table_annotation(tableName, subcol_dict):\n",
    "    tableD = dataloader.read_table(tableName)\n",
    "    print(tableD)\n",
    "    annotation_table, NE_Score = subcol_dict[tableName + \".csv\"]\n",
    "    print(annotation_table)\n",
    "    ### Learning phase of TableMiner+\n",
    "    tableLearning = TableLearning(tableD, NE_column=NE_Score)\n",
    "    ### Perform NE-Column interpretation - the UPDATE phase\n",
    "    tableLearning.table_learning()\n",
    "    updatePhase(tableLearning)\n",
    "    return tableLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def store_learning(table, learning, dict_path, dict_name):\n",
    "    target_file = os.path.join(dict_path, dict_name)\n",
    "    if os.path.isfile(target_file):\n",
    "        with open(target_file, 'rb') as file:\n",
    "            dict_annotation = pickle.load(file)\n",
    "    else:\n",
    "        dict_annotation = {}\n",
    "    dict_annotation[table] = learning\n",
    "    with open(target_file, 'wb') as file:\n",
    "        pickle.dump(dict_annotation, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "searched_table = table_domains[0]\n",
    "learning = table_annotation(searched_table, SubjectCol_dict)\n",
    "store_learning(searched_table, learning, \"Result\", \"annotationDict.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### check the annotation of column and column header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "annotation_class = learning.get_annotation_class()\n",
    "for column_index, learning_class in annotation_class.items():\n",
    "    column = table_df.iloc[:,column_index]\n",
    "    cellAnnotation  = learning_class.get_cell_annotation()\n",
    "    ColumnSemantics = learning_class.get_winning_concepts()\n",
    "    df = pd.concat([column, cellAnnotation], axis=1)\n",
    "    print(f\"column and Cell annotation of the column:\\n{df}\\n\")\n",
    "    print(f\"Column {column.name} semantic type: {ColumnSemantics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Part 4: Schema Inference\n",
    "### Framework Overview -- Starmie\n",
    "#### Input dataset: all 200 tables covering 13 specific domains\n",
    "Embedding methods: Starmie\n",
    "Table class inference method: Hierarchical clustering\n",
    "Smilairity metric: Average column embeddings of each table\n",
    "Type of result table clusters: the most frequently appeared class in the ground truth in each cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Use clustering on table's embeddings to detect the types/domains' of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import clustering  as c\n",
    "\"\"\"\n",
    "Generation of Embeddings: Please run the ./starmie/cmd.sh script to generate the tables' embeddings \n",
    "We uploaded the sample embeddings generated by this script to the Result path.\n",
    "\"\"\"\n",
    "clustering_result = c.typeInference(\"Result/tableEmbeddings.pkl\", \"Agglomerative\", numEstimate=13)\n",
    "index_cluster = [i for i, tables in clustering_result.items() if \"T2DV2_122.csv\" in tables]\n",
    "# The overall result of clustering in a dataframe, includes cluster id, GT label, size, precision,\n",
    "# Ranked by precision\n",
    "result_precision = c.result_precision(clustering_result)\n",
    "print(result_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Check the last second clusters' score\n",
    "print(\"\\n\",result_precision.iloc[-2])\n",
    "# Check inside the cluster, what kind of table does it contain\n",
    "checked_cluster = clustering_result[result_precision.iloc[-2][\"cluster id\"]]\n",
    "innerInfo = c.inner_cluster(checked_cluster)\n",
    "print(innerInfo)\n",
    "# select random two tables inside the sample cluster; check and compare their column headers\n",
    "# to find the difference/similarities between then\n",
    "rows = c.sample_tables_cluster(innerInfo)\n",
    "first_table = dataloader.read_table(rows.iloc[0, 0][:-4])\n",
    "second_table = dataloader.read_table(rows.iloc[1, 0][:-4])\n",
    "print(first_table.columns,\"\\n\",second_table.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "##### Use clustering on column embeddings to detect column's type\n",
    "column type inference: using hierarchical clustering on the column embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "### Cluster all column embeddings under specified domain\n",
    "column_clustering = c.conceptualAttri(os.path.join(c.current_dir_path, \"Datasets\"),\n",
    "                os.path.join(c.current_dir_path, \"Result/tableEmbeddings.pkl\"),\n",
    "                clustering_method=\"Agglomerative\",\n",
    "                domain=\"VideoGame\",\n",
    "                numEstimate=13)\n",
    "\n",
    "check_table = \"T2DV2_122\"\n",
    "\n",
    "for column_index, column_clusters in column_clustering.items():\n",
    "    for i in  column_clusters:\n",
    "        if check_table in i:\n",
    "            column = [i for i in column_clusters if check_table in i][0]\n",
    "            print(f\"Cluster has {column} \\n\",column_clusters, \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
