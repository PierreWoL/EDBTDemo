{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_len = 80345\n"
     ]
    }
   ],
   "source": [
    "# load tables\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# re-build indices\n",
    "sato_path = \"/nfs/users/yuliang/ssl-em/column_type_detection/data\"\n",
    "index = {}\n",
    "max_len = 64\n",
    "\n",
    "for sid in range(5):\n",
    "    path = os.path.join(sato_path, \"sato_cv_%d.csv\" % sid)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for table_id, col_idx, data, cls in zip(df['table_id'], df['col_idx'], df['data'], df['class']):\n",
    "        tokens = data.split(' ')\n",
    "        data = ' '.join(tokens[:max_len])\n",
    "        index[data] = (table_id, col_idx, cls)\n",
    "\n",
    "print('index_len =', len(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:13<00:00, 383.83it/s]\n",
      "100%|██████████| 2500/2500 [00:05<00:00, 417.63it/s]\n",
      "100%|██████████| 2500/2500 [00:04<00:00, 533.09it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/nfs/users/yuliang/ssl-em/column_type_detection\"\n",
    "\n",
    "datasets = []\n",
    "all_tables = {}\n",
    "table_lists = []\n",
    "\n",
    "for fn in ['train.txt', 'valid.txt', 'test.txt']:\n",
    "    path = os.path.join(dataset_path, fn)\n",
    "    rows = []\n",
    "    labels = []\n",
    "    output_df = {'l_table_id': [],\n",
    "                 'r_table_id': [],\n",
    "                 'l_column_id': [],\n",
    "                 'r_column_id': [],\n",
    "                 'l_ori_table_id': [],\n",
    "                 'r_ori_table_id': [],\n",
    "                 'l_column_type': [],\n",
    "                 'r_column_type': [],\n",
    "                 'match': []}\n",
    "\n",
    "    for line in tqdm(open(path).readlines()):\n",
    "        left, right, label = line.strip().split('\\t')\n",
    "        labels.append(label)\n",
    "\n",
    "        features = []\n",
    "        for text, prefix in zip([left, right], [\"l_\", \"r_\"]):\n",
    "            ori_table_id = index[text][0]\n",
    "            col_idx = index[text][1]\n",
    "            cls = index[text][2]\n",
    "            if ori_table_id not in all_tables:\n",
    "                table_path = os.path.join(\"/nfs/users/yuliang/table_data/viznet_tables/\", index[text][0])\n",
    "                df = pd.read_csv(table_path, index_col=[0])\n",
    "                # new table id and the DataFrame\n",
    "                all_tables[ori_table_id] = (len(all_tables), df)\n",
    "                table_lists.append(df)\n",
    "            \n",
    "            table_id, df = all_tables[ori_table_id]\n",
    "            output_df[prefix + \"table_id\"].append(table_id)\n",
    "            output_df[prefix + \"column_id\"].append(col_idx)\n",
    "            output_df[prefix + \"ori_table_id\"].append(ori_table_id)\n",
    "            output_df[prefix + \"column_type\"].append(cls)\n",
    "        \n",
    "        output_df['match'].append(1 if output_df['l_column_type'][-1] == output_df['r_column_type'][-1] else 0)\n",
    "    \n",
    "    # output\n",
    "    output_path = os.path.join(dataset_path, fn.replace('.txt', '.csv'))\n",
    "    output_df = pd.DataFrame(output_df)\n",
    "    output_df.to_csv(output_path, index=False)\n",
    "\n",
    "# output all_tables\n",
    "for idx, table in enumerate(table_lists):\n",
    "    output_path = os.path.join(dataset_path, 'table_%d.csv' % idx)\n",
    "    table.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the viznet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23820/23820 [00:20<00:00, 1151.86it/s]\n",
      "100%|██████████| 23877/23877 [00:17<00:00, 1346.04it/s]\n",
      "100%|██████████| 23893/23893 [00:17<00:00, 1355.23it/s]\n",
      "100%|██████████| 23783/23783 [00:15<00:00, 1508.37it/s]\n",
      "100%|██████████| 23987/23987 [00:14<00:00, 1677.27it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/nfs/users/yuliang/SDD/data/viznet\"\n",
    "sato_path = \"/nfs/users/yuliang/ssl-em/column_type_detection/data\"\n",
    "\n",
    "datasets = []\n",
    "all_tables = {}\n",
    "table_lists = []\n",
    "all_columns = {'table_id': [],\n",
    "               'ori_table_id': [],\n",
    "               'column_id': [],\n",
    "               'class': []}\n",
    "\n",
    "# re-build indices\n",
    "index = {}\n",
    "max_len = 64\n",
    "\n",
    "for sid in range(5):\n",
    "    path = os.path.join(sato_path, \"sato_cv_%d.csv\" % sid)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for table_id, col_idx, data, cls in zip(df['table_id'], df['col_idx'], df['data'], df['class']):\n",
    "        tokens = data.split(' ')\n",
    "        data = ' '.join(tokens[:max_len])\n",
    "        index[data] = (table_id, col_idx, cls)\n",
    "\n",
    "\n",
    "for sid in range(5):\n",
    "    path = os.path.join(sato_path, \"sato_cv_%d.csv\" % sid)\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    for data, cls in tqdm(zip(df['data'], df['class']), total=len(df)):\n",
    "        tokens = data.split(' ')\n",
    "        data = ' '.join(tokens[:max_len])\n",
    "        \n",
    "        ori_table_id = index[data][0]\n",
    "        col_idx = index[data][1]\n",
    "\n",
    "        if ori_table_id not in all_tables:\n",
    "            table_path = os.path.join(\"/nfs/users/yuliang/table_data/viznet_tables/\", ori_table_id)\n",
    "            df = pd.read_csv(table_path, index_col=[0])\n",
    "            # new table id and the DataFrame\n",
    "            table_id = len(all_tables)\n",
    "            all_tables[ori_table_id] = (len(all_tables), df)\n",
    "            table_lists.append(df)\n",
    "        else:\n",
    "            table_id = all_tables[ori_table_id][0]\n",
    "        \n",
    "        all_columns['table_id'].append(table_id)\n",
    "        all_columns['ori_table_id'].append(ori_table_id)\n",
    "        all_columns['column_id'].append(col_idx)\n",
    "        all_columns['class'].append(cls)\n",
    "\n",
    "all_columns = pd.DataFrame(all_columns) \n",
    "all_columns.to_csv(os.path.join(dataset_path, 'test.csv'), index=False)\n",
    "\n",
    "# output all_tables\n",
    "for idx, table in enumerate(table_lists):\n",
    "    output_path = os.path.join(dataset_path, 'tables', 'table_%d.csv' % idx)\n",
    "    table.to_csv(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "path = '../python/'\n",
    "\n",
    "# load data\n",
    "column_vectors, labels = pickle.load(open(\"../data/viznet/multi_column/column_vectors.pkl\", \"rb\"))\n",
    "# pairs = pickle.load(open(\"../python/column_pairs.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 20035/119360 [00:00<00:01, 96277.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  3 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 48508/119360 [00:00<00:00, 92311.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  3 2\n",
      "error:  3 2\n",
      "error:  3 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 77231/119360 [00:00<00:00, 94977.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  3 2\n",
      "error:  3 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 97221/119360 [00:01<00:00, 97580.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error:  3 3\n",
      "error:  3 2\n",
      "error:  3 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119360/119360 [00:01<00:00, 95355.72it/s]\n"
     ]
    }
   ],
   "source": [
    "# sherlock and sato\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "testset = pd.read_csv(\"../data/viznet/test.csv.full\")\n",
    "sherlock_sato_features = pickle.load(open('sato/sato_features.pkl', 'rb'))\n",
    "\n",
    "sherlock_features = []\n",
    "sato_features = []\n",
    "idx = 0\n",
    "\n",
    "# for table_id, column_id in tqdm(zip(testset['table_id'], testset['column_id']), total=len(testset)):\n",
    "#     num_column = len(sherlock_sato_features[idx][0])\n",
    "#     real_num_column = len(pd.read_csv(\"../data/viznet/tables/table_%d.csv\" % table_id).columns)\n",
    "#     print(idx, num_column, real_num_column)\n",
    "#     idx += 1\n",
    "\n",
    "\n",
    "for table_id, column_id in tqdm(zip(testset['table_id'], testset['column_id']), total=len(testset)):\n",
    "    if column_id >= len(sherlock_sato_features[idx][0]):\n",
    "        print(\"error: \", column_id, len(sherlock_sato_features[idx][0]))\n",
    "        column_id = len(sherlock_sato_features[idx][0]) - 1\n",
    "\n",
    "    sherlock_feature = sherlock_sato_features[idx][0][column_id]\n",
    "    sato_feature = np.concatenate([sherlock_feature, sherlock_sato_features[idx][1]])\n",
    "    sherlock_features.append(sherlock_feature)\n",
    "    sato_features.append(sato_feature)\n",
    "    idx += 1\n",
    "\n",
    "pickle.dump(sherlock_features, open(\"../data/viznet/sherlock/column_vectors.pkl\", \"wb\"))\n",
    "pickle.dump(sato_features, open(\"../data/viznet/sato/column_vectors.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import deque, Counter\n",
    "\n",
    "\n",
    "def blocked_matmul(mata, matb,\n",
    "                   threshold=None,\n",
    "                   k=None,\n",
    "                   batch_size=512):\n",
    "    \"\"\"Find the most similar pairs of vectors from two matrices (top-k or threshold)\n",
    "\n",
    "    Args:\n",
    "        mata (np.ndarray): the first matrix\n",
    "        matb (np.ndarray): the second matrix\n",
    "        threshold (float, optional): if set, return all pairs of cosine\n",
    "            similarity above the threshold\n",
    "        k (int, optional): if set, return for each row in matb the top-k\n",
    "            most similar vectors in mata\n",
    "        batch_size (int, optional): the batch size of each block\n",
    "    \n",
    "    Returns:\n",
    "        list of tuples: the pairs of similar vectors' indices and the similarity\n",
    "    \"\"\"\n",
    "    mata = np.array(mata)\n",
    "    matb = np.array(matb)\n",
    "    results = []\n",
    "    for start in tqdm(range(0, len(matb), batch_size)):\n",
    "        block = matb[start:start+batch_size]\n",
    "        sim_mat = np.matmul(mata, block.transpose())\n",
    "        if k is not None:\n",
    "            indices = np.argpartition(-sim_mat, k, axis=0)\n",
    "            for row in indices[:k]:\n",
    "                for idx_b, idx_a in enumerate(row):\n",
    "                    idx_b += start\n",
    "                    results.append((idx_a, idx_b, sim_mat[idx_a][idx_b-start]))\n",
    "        elif threshold is not None:\n",
    "            indices = np.argwhere(sim_mat >= threshold)\n",
    "            for idx_a, idx_b in indices:\n",
    "                idx_b += start\n",
    "                results.append((idx_a, idx_b, sim_mat[idx_a][idx_b-start]))\n",
    "    return results\n",
    "\n",
    "\n",
    "def connected_components(pairs, cluster_size=50):\n",
    "    \"\"\"Helper function for computing the connected components\n",
    "    \"\"\"\n",
    "    edges = {}\n",
    "    pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "    for left, right, _ in pairs:\n",
    "        if left not in edges:\n",
    "            edges[left] = []\n",
    "        if right not in edges:\n",
    "            edges[right] = []\n",
    "            \n",
    "        edges[left].append(right)\n",
    "        edges[right].append(left)\n",
    "    \n",
    "    # print('num nodes =', len(edges))\n",
    "    all_ccs = []\n",
    "    used = set([])\n",
    "    for start in edges:\n",
    "        if start in used:\n",
    "            continue\n",
    "        used.add(start)\n",
    "        cc = [start]\n",
    "        \n",
    "        queue = deque([start])\n",
    "        while len(queue) > 0:\n",
    "            u = queue.popleft()\n",
    "            for v in edges[u]:\n",
    "                if v not in used:\n",
    "                    cc.append(v)\n",
    "                    used.add(v)\n",
    "                    queue.append(v)\n",
    "                    if len(cc) >= cluster_size:\n",
    "                        break\n",
    "            \n",
    "            if len(cc) >= cluster_size:\n",
    "                break\n",
    "        \n",
    "        all_ccs.append(cc)\n",
    "        # print(cc)\n",
    "    return all_ccs\n",
    "\n",
    "\n",
    "def evaluate_clustering(vectors, labels):\n",
    "    \"\"\"Evaluate column clustering on input column vectors.\n",
    "    \"\"\"\n",
    "    # normalize the vectors\n",
    "    vectors = np.array(vectors)\n",
    "    vectors /= np.linalg.norm(vectors, axis=-1)[:, np.newaxis]\n",
    "\n",
    "    # top k matching columns\n",
    "    pairs = blocked_matmul(vectors, vectors,\n",
    "                           k=20,\n",
    "                           batch_size=4096)\n",
    "\n",
    "    # dump the clustering results\n",
    "    pickle.dump(pairs, open('column_pairs.pkl', 'wb'))\n",
    "\n",
    "    # run column clustering algorithm\n",
    "    ccs = connected_components(pairs)\n",
    "\n",
    "    # dump the clustering results\n",
    "    pickle.dump(ccs, open('clusters.pkl', 'wb'))\n",
    "\n",
    "    # compute purity\n",
    "    purity = []\n",
    "    total = 0\n",
    "    for cc in ccs:\n",
    "        cnt = Counter()\n",
    "        for column_id in cc:\n",
    "            label = labels[column_id]\n",
    "            cnt[label] += 1\n",
    "        purity.append(cnt.most_common(1)[0][1])\n",
    "        total += len(cc)\n",
    "    purity = np.sum(purity) / total\n",
    "\n",
    "    return {\"num_clusters\": len(ccs), \n",
    "            \"avg_cluster_size\": np.mean([len(cc) for cc in ccs]),\n",
    "            \"purity\": purity}\n",
    "\n",
    "# for method in ['sherlock', 'sato', 'single_column', 'multi_column']:\n",
    "#     column_vectors = pickle.load(open('../data/viznet/%s/column_vectors.pkl' % method, \"rb\"))\n",
    "#     res = evaluate_clustering(column_vectors, labels)\n",
    "#     print(res)\n",
    "#     os.system('mv *.pkl ../data/viznet/%s/' % method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'sato', 'num_clusters': 2456, 'avg_cluster_size': 48.59934853420195, 'purity': 0.37356735924932977}\n",
      "{'method': 'sherlock', 'num_clusters': 2395, 'avg_cluster_size': 49.83716075156576, 'purity': 0.3050351876675603}\n",
      "{'method': 'multi_column', 'num_clusters': 2297, 'avg_cluster_size': 51.96343056160209, 'purity': 0.5118800268096515}\n",
      "{'method': 'single_column', 'num_clusters': 9252, 'avg_cluster_size': 12.900994379593602, 'purity': 0.20379524128686327}\n"
     ]
    }
   ],
   "source": [
    "def compute_purity(ccs):\n",
    "    purity = []\n",
    "    total = 0\n",
    "    for cc in ccs:\n",
    "        cnt = Counter()\n",
    "        for column_id in cc:\n",
    "            label = labels[column_id]\n",
    "            cnt[label] += 1\n",
    "        purity.append(cnt.most_common(1)[0][1])\n",
    "        total += len(cc)\n",
    "    purity = np.sum(purity) / total\n",
    "    return purity\n",
    "\n",
    "def tune_cluster_size(pairs, target=50):\n",
    "    left = 0\n",
    "    right = 5000\n",
    "    min_diff = 1e6\n",
    "    res_ccs = []\n",
    "\n",
    "    while right - left > 10:\n",
    "        mid = (left + right) // 2\n",
    "        ccs = connected_components(pairs, cluster_size=mid)\n",
    "        avg_size = np.mean([len(cc) for cc in ccs])\n",
    "        if abs(avg_size - target) < min_diff:\n",
    "            min_diff = abs(avg_size - target)\n",
    "            res_ccs = ccs\n",
    "\n",
    "        # print(mid, avg_size)\n",
    "        if avg_size > target:\n",
    "            right = mid\n",
    "        else:\n",
    "            left = mid\n",
    "        # purity = compute_purity(ccs)\n",
    "        \n",
    "    purity = compute_purity(res_ccs)\n",
    "    return res_ccs, purity\n",
    "\n",
    "\n",
    "for model in ['sato', 'sherlock', 'multi_column', 'single_column']:\n",
    "    pairs = pickle.load(open(\"../data/viznet/%s/column_pairs.pkl\" % model, \"rb\"))\n",
    "    ccs, purity = tune_cluster_size(pairs)\n",
    "    res = {\"method\": model,\n",
    "            \"num_clusters\": len(ccs), \n",
    "            \"avg_cluster_size\": np.mean([len(cc) for cc in ccs]),\n",
    "            \"purity\": purity}\n",
    "    print(res)\n",
    "\n",
    "    # for cluster_size in [25, 50, 75, 100, 150, 200]:\n",
    "    #     ccs = connected_components(pairs, cluster_size=cluster_size)\n",
    "\n",
    "    #     # compute purity\n",
    "    #     purity = compute_purity(ccs)\n",
    "    #     res = {\"method\": model,\n",
    "    #             \"num_clusters\": len(ccs), \n",
    "    #             \"avg_cluster_size\": np.mean([len(cc) for cc in ccs]),\n",
    "    #             \"purity\": purity}\n",
    "    #     print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist ---- 1. I Don't Give A ...; 2. I'm The Kinda; 3. I U She; 4. Kick It [featuring Iggy Pop]; 5. Operate\n",
      "artist ---- 1. Spoken Intro; 2. The Court; 3. Maze; 4. Girl Talk; 5. A La Mode\n",
      "artist ---- 1. Street Fighting Man; 2. Gimme Shelter; 3. (I Can't Get No) Satisfaction; 4. The Last Time; 5. Jumpin' Jack Flash\n",
      "artist ---- 1. Angel of the Morning; 2. Shot Full of Love; 3. Ride 'Em Cowboys; 4. Queen of Hearts; 5. River of Love\n",
      "artist ---- 1. New Wave; 2. Up The Cuts; 3. Thrash Unreal; 4. White People For Peace; 5. Stop!\n",
      "artist ---- 1. Trigger Happy; 2. Sentimental Fool; 3. I Didn't Know That You Cared; 4. Love Ruins Everything; 5. Baby\n",
      "artist ---- 1. You; 2. Creep; 3. How Do You?; 4. Stop Whispering; 5. Thinking About You\n",
      "artist ---- 1. Buena; 2. Honey White; 3. You Speak My Language; 4. Cure for Pain; 5. Candy\n",
      "artist ---- 1. Mr. Grieves; 2. Crackity Jones; 3. La La Love You; 4. No. 13 Baby; 5. There Goes My Gun\n",
      "artist ---- 1. Street Fighting Man; 2. Gimme Shelter; 3. (I Can't Get No) Satisfaction; 4. The Last Time; 5. Jumpin' Jack Flash\n",
      "---------------------------------\n",
      "type ---- Emerson Elementary School; Banneker Elementary School; Silver City Elementary School; New Stanley Elementary School; Frances Willard Elementary School\n",
      "type ---- Choctawhatchee Senior High School; Fort Walton Beach High School; Ami Kids Emerald Coast; Gulf Coast Christian School; Adolescent Substance Abuse\n",
      "city ---- Chilton; Stoughton\n",
      "name ---- Crasnier-Mednansky, Martine; Park, Maxwell; Studley, William; Saier, Milton\n",
      "type ---- Roosevelt High School; Karen Wagner High School; Thompson Center\n",
      "type ---- Oak Park and River Forest High School; Harbor Academy Reg Safe Sch Prg; Fenwick High School; Trinity High School\n",
      "type ---- Emerson Elementary School; Banneker Elementary School; Silver City Elementary School; New Stanley Elementary School; Frances Willard Elementary School\n",
      "type ---- Roosevelt School; Gwendolyn Brooks Middle School; Percy Julian Middle School; St. Luke Catholic School; Learning Network\n",
      "type ---- Sumner Academy Of Arts and Science; Wyandotte High School; J C Harmon High School; School For Blind High; Bishop Ward High School\n",
      "type ---- Emerson Elementary School; Banneker Elementary School; Silver City Elementary School; New Stanley Elementary School; Frances Willard Elementary School\n",
      "---------------------------------\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "day ---- Sept. 1; Sept. 7; Sept. 22; Sept. 29; Oct. 5\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "description ---- Fri Sep 11,2015 3:30 PM (CST); Fri Sep 11,2015 6:00 PM (CST); Sat Sep 12,2015 10:00 AM (CST); Sat Sep 12,2015 12:00 PM (CST); Sat Sep 12,2015 5:30 PM (CST)\n",
      "---------------------------------\n",
      "name ---- People's Grocery Co-op Exchange; Prairieland Market; The Merc (Community Mercantile); Topeka Natural Food Co-op\n",
      "name ---- People's Grocery Co-op Exchange; Prairieland Market; The Merc (Community Mercantile); Topeka Natural Food Co-op\n",
      "name ---- People's Grocery Co-op Exchange; Prairieland Market; The Merc (Community Mercantile); Topeka Natural Food Co-op\n",
      "name ---- People's Grocery Co-op Exchange; Prairieland Market; The Merc (Community Mercantile); Topeka Natural Food Co-op\n",
      "name ---- Amazing Grains; BisMan Community Food Cooperative; Bowdon Locker & Grocery; Prairie Roots Food Co-op\n",
      "name ---- Apples Street Market; Bexley Natural Market; Clintonville Community Market; Kent Natural Foods Co-op; MOON Co-op Natural Foods Market\n",
      "name ---- Fiddleheads Food Co-op; Northwest Corner Co-op; The Local Beet Co-op; Willimantic Food Co-op\n",
      "name ---- Fiddleheads Food Co-op; Northwest Corner Co-op; The Local Beet Co-op; Willimantic Food Co-op\n",
      "name ---- Bread & Roses Food Cooperative; Citizens Co-op; Community Harvest Market; Ever'man Cooperative Grocery & Cafe; New Leaf Market\n",
      "name ---- Bread & Roses Food Cooperative; Citizens Co-op; Community Harvest Market; Ever'man Cooperative Grocery & Cafe; New Leaf Market\n",
      "---------------------------------\n",
      "address ---- 1930 Lagen St, Dubuque IA; 3860 Short St, Dubuque IA; 3962 Aurora St, Dubuque IA; 2222 Saint Celia St, Dubuque IA; 1676 Amy Ct, Dubuque IA\n",
      "address ---- 43 Whitney Rd, Mystic CT; 87 Quaker Farm Rd, Mystic CT; 171 Lambtown Rd, Mystic CT; 1827 Gold Star Hwy, Groton CT; 25 Nantucket Dr, Mystic CT\n",
      "address ---- 3418 Magnolia Way, Broadview Heights OH; 3397 Magnolia Way, Broadview Heights OH; 3080 Osage Way, Broadview Heights OH; 13741 Monica Dr, North Royalton OH; 13021 Eagle Chase, North Royalton OH\n",
      "address ---- 2762 Riverwood Ln, Jacksonville FL; 1804 Lorimier Rd, Jacksonville FL; 1639 Lorimier Rd, Jacksonville FL; 1705 Lorimier Rd, Jacksonville FL; 2757 White Oak Ln, Jacksonville FL\n",
      "address ---- 7232 E Mckinley St, Scottsdale AZ; 519 N 73rd Pl, Scottsdale AZ; 7308 E Polk St, Scottsdale AZ; 713 N 74th St, Scottsdale AZ; 24 E Mckinley Cir, Tempe AZ\n",
      "address ---- 5725 N Depauw St, Portland OR; 6126 N Superior St, Portland OR; 7129 N Buchanan Ave, Portland OR; 9006 N Ida Ave, Portland OR; 4925 N Princeton St, Portland OR\n",
      "address ---- 31 Lawndale Ave, Lebanon OH; 18 Lawndale Ave, Lebanon OH; 908 Hartz Dr, Lebanon OH; 917 Stanwood Dr, Lebanon OH; 923 Birchwood Dr, Lebanon OH\n",
      "address ---- 1721 Papillon St, North Port FL; 4113 Wabasso Ave, North Port FL; 3681 Wayward Ave, North Port FL; 1118 N Salford Blvd, North Port FL; 2057 Bendix Ter, North Port FL\n",
      "address ---- 5 Brand Rd, Toms River NJ; 40 12th St, Toms River NJ; 75 Sea Breeze Rd, Toms River NJ; 98 Oak Tree Ln, Toms River NJ; 67 16th St, Toms River NJ\n",
      "address ---- 652 Martha St, Montgomery AL; 3184 Lexington Rd, Montgomery AL; 120 S Lewis St, Montgomery AL; 1812 W 2nd St #OP, Montgomery AL; 3582 Southview Ave, Montgomery AL\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# visualize each cluster\n",
    "\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "dataset_path = '/nfs/users/yuliang/SDD/data/viznet/test.csv.full'\n",
    "ccs = pickle.load(open(\"../data/viznet/multi_column/clusters.pkl\", \"rb\"))\n",
    "testset = pd.read_csv(dataset_path)\n",
    "\n",
    "def show_cc(ccs, idx):\n",
    "    for cid in ccs[idx][:10]:\n",
    "        table_id = testset['table_id'][cid]\n",
    "        column_id = testset['column_id'][cid]\n",
    "        label = testset['class'][cid]\n",
    "\n",
    "        table = pd.read_csv('/nfs/users/yuliang/SDD/data/viznet/tables/table_%d.csv' % table_id)\n",
    "        value = '; '.join(table[table.columns[column_id]][:5].astype(str))\n",
    "\n",
    "        # print(label, cid, table_id, column_id, '----', value)\n",
    "        print(label, '----', value)\n",
    "    print(\"---------------------------------\")\n",
    "\n",
    "show_cc(ccs, 10)\n",
    "show_cc(ccs, 35)\n",
    "show_cc(ccs, 57)\n",
    "show_cc(ccs, 69)\n",
    "show_cc(ccs, 78)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a4defd7b6bdd6eb378ea4d7dbbc33dd84cc0def60f9189b1f6f216e34c2c378c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('sdd')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
